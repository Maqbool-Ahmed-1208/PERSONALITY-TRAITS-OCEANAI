{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10545318,"sourceType":"datasetVersion","datasetId":6524649}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"vscode":{"interpreter":{"hash":"e1a611bfe618ddced957fae6e2b829e3db4b5e8138885cb3bfce781d42e2449a"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FEATURES IN KAGGLE OUTPUT ","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# #copy any csv file path from outputs to display dataframe \n# dff = pd.read_csv('/kaggle/working/feature_extraction/train_4_vf/p47.csv')\n# dff.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python test/unit_tests_fi_en.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pytest test/unit_tests_fi_en.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cd OCEANAI","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import pandas as pd\n# from oceanai.modules.lab.architectures.audio_architectures import audio_model_hc  # or any other model you use","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize the model\n# model = audio_model_hc(input_size=25)  # Ensure the input size matches your data\n# model.eval()  # Set the model to evaluation mode\n\n# # Optionally, load pre-trained weights if available\n# model.load_state_dict(torch.load('models/ahc_mupta_2022-06-18_08-32-05.pth'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def extract_and_save_features(audio_data, csv_file_name):\n#     \"\"\"\n#     Extract features from audio data and save to a CSV file.\n\n#     :param audio_data: Tensor of shape (batch_size, time_steps, input_size)\n#     :param csv_file_name: Output CSV file path\n#     \"\"\"\n#     # Extract features using the model\n#     with torch.no_grad():\n#         features = model.extract_features(audio_data)  # Extract features\n    \n#     # Convert to numpy for saving\n#     features_np = features.cpu().numpy()\n    \n#     # Create a DataFrame and save to CSV\n#     df = pd.DataFrame(features_np)\n#     df.to_csv(csv_file_name, index=False)\n#     print(f\"Features saved to {csv_file_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def preprocess_individual_audio(file_path, n_mels=25, sr=16000, duration=2):\n#     \"\"\"\n#     Preprocess an individual's audio file into a fixed-length mel spectrogram tensor.\n\n#     :param file_path: Path to the audio file\n#     :param n_mels: Number of mel bands (must match LSTM input size)\n#     :param sr: Sampling rate\n#     :param duration: Fixed duration (in seconds) for feature extraction\n#     :return: PyTorch tensor of shape (1, time_steps, n_mels)\n#     \"\"\"\n#     import librosa\n#     import numpy as np\n#     import torch\n\n#     # Load the audio as mono\n#     y, sr = librosa.load(file_path, sr=sr, mono=True)\n\n#     # Normalize audio length by padding/trimming\n#     max_length = int(sr * duration)  # Fixed duration in samples\n#     y = librosa.util.fix_length(y, size=max_length)\n\n#     # Compute mel spectrogram\n#     mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n\n#     # Convert power spectrogram to dB scale\n#     mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n\n#     # Return tensor with shape (1, time_steps, n_mels)\n#     return torch.tensor(mel_spectrogram_db.T).unsqueeze(0).float()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# audio_tensor = preprocess_individual_audio('video_MuPTA/test/speaker_01_center_83.wav')\n# print(audio_tensor.shape)  # Expected shape: (1, time_steps, 25)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract_and_save_features(audio_tensor, 'audio_hc_features/mu_audio_hc_features_speaker_01.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os  # Взаимодействие с файловой системой\n# import sys # Доступ к некоторым переменным и функциям Python\n# from oceanai.modules.lab.build import Run\n# PATH_TO_SOURCE = os.path.abspath(os.path.dirname(globals()['_dh'][0]))\n# PATH_TO_ROOT = os.path.join(PATH_TO_SOURCE, '..', '..', '..')\n\n# sys.path.insert(0, os.path.abspath(PATH_TO_ROOT))\n\n\n# _b5 = Run(\n#     lang = 'ru', # Язык\n#     color_simple = '#333', # Цвет обычного текста (шестнадцатеричный код)\n#     color_info = '#1776D2', # Цвет текста содержащего информацию (шестнадцатеричный код)\n#     color_err = '#FF0000', # Цвет текста содержащего ошибку (шестнадцатеричный код)\n#     color_true = '#008001', # Цвет текста содержащего положительную информацию (шестнадцатеричный код)\n#     bold_text = True, # Жирное начертание текста\n#     num_to_df_display = 30, # Количество строк для отображения в таблицах\n#     text_runtime = 'Время выполнения', # Текст времени выполнения\n#     metadata = True # Отображение информации о библиотеке\n# )\n# _b5.libs_vers(runtime = True, run = True)\n# res_load_audio_model_hc = _b5.load_audio_model_hc(\n#     show_summary = False, # Отображение сформированной нейросетевой архитектуры модели\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )\n\n\n# # Настройки ядра\n# _b5.path_to_save_ = './models' # Директория для сохранения файла\n# _b5.chunk_size_ = 2000000 # Размер загрузки файла из сети за 1 шаг\n\n# url = _b5.weights_for_big5_['audio']['fi']['hc']['googledisk']\n\n# res_load_audio_model_weights_hc = _b5.load_audio_model_weights_hc(\n#     url = url, # Полный путь к файлу с весами нейросетевой модели\n#     force_reload = True, # Принудительная загрузка файла с весами нейросетевой модели из сети\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )\n\n\n# res_load_audio_model_nn = _b5.load_audio_model_nn(\n#     show_summary = False, # Отображение сформированной нейросетевой архитектуры модели\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )\n\n\n# # Настройки ядра\n# _b5.path_to_save_ = './models' # Директория для сохранения файла\n# _b5.chunk_size_ = 2000000 # Размер загрузки файла из сети за 1 шаг\n\n# url = _b5.weights_for_big5_['audio']['fi']['nn']['googledisk']\n\n# res_load_audio_model_weights_nn = _b5.load_audio_model_weights_nn(\n#     url = url, # Полный путь к файлу с весами нейросетевой модели\n#     force_reload = False, # Принудительная загрузка файла с весами нейросетевой модели из сети\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )\n\n\n# res_load_audio_models_b5 = _b5.load_audio_models_b5(\n#     show_summary = False, # Отображение сформированной нейросетевой архитектуры модели\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )\n\n# # Настройки ядра\n# _b5.path_to_save_ = './models' # Директория для сохранения файла\n# _b5.chunk_size_ = 2000000 # Размер загрузки файла из сети за 1 шаг\n\n# url_openness = _b5.weights_for_big5_['audio']['fi']['b5']['openness']['googledisk']\n# url_conscientiousness = _b5.weights_for_big5_['audio']['fi']['b5']['conscientiousness']['googledisk']\n# url_extraversion = _b5.weights_for_big5_['audio']['fi']['b5']['extraversion']['googledisk']\n# url_agreeableness = _b5.weights_for_big5_['audio']['fi']['b5']['agreeableness']['googledisk']\n# url_non_neuroticism = _b5.weights_for_big5_['audio']['fi']['b5']['non_neuroticism']['googledisk']\n\n# res_load_audio_models_weights_b5 = _b5.load_audio_models_weights_b5(\n#     url_openness = url_openness, # Открытость опыту\n#     url_conscientiousness = url_conscientiousness, # Добросовестность\n#     url_extraversion = url_extraversion, # Экстраверсия\n#     url_agreeableness = url_agreeableness, # Доброжелательность\n#     url_non_neuroticism = url_non_neuroticism, # Эмоциональная стабильность\n#     force_reload = False, # Принудительная загрузка файла с весами нейросетевой модели из сети\n#     out = True, # Отображение\n#     runtime = True, # Подсчет времени выполнения\n#     run = True # Блокировка выполнения\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# # Настройки ядра\n# _b5.path_to_dataset_ = 'Databases/train_1' # Директория набора данных\n# # Директории не входящие в выборку\n# _b5.ignore_dirs_ = []\n# # Названия ключей для DataFrame набора данных\n# _b5.keys_dataset_ = ['Path', 'Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Non-Neuroticism']\n# _b5.ext_ = ['.mp4'] # Расширения искомых файлов\n# _b5.path_to_logs_ = './logs' # Директория для сохранения LOG файлов\n\n# # Полный путь к файлу с верными предсказаниями для подсчета точности\n# url_accuracy = _b5.true_traits_['fi']['googledisk']\n\n# res_get_audio_union_predictions = _b5.get_audio_union_predictions(\n#     depth = 1,         # Глубина иерархии для получения аудио и видеоданных\n#     recursive = False, # Рекурсивный поиск данных\n#     sr = 44100,        # Частота дискретизации\n#     window = 2,        # Размер окна сегмента сигнала (в секундах)\n#     step = 1,          # Шаг сдвига окна сегмента сигнала (в секундах)\n#     accuracy = True,   # Вычисление точности\n#     url_accuracy = url_accuracy,\n#     logs = True,      # При необходимости формировать LOG файл\n#     out = True,        # Отображение\n#     runtime = True,    # Подсчет времени выполнения\n#     run = True         # Блокировка выполнения\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install -r docs/requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import cv2\n# import numpy as np\n\n# def segment_video(video_path, segment_duration=2, overlap=1, fps=5):\n#     \"\"\"\n#     Segments a video into overlapping chunks.\n\n#     :param video_path: Path to the video file\n#     :param segment_duration: Duration of each segment (seconds)\n#     :param overlap: Overlap between segments (seconds)\n#     :param fps: Frames per second to sample\n#     :return: List of video segments as numpy arrays\n#     \"\"\"\n#     cap = cv2.VideoCapture(video_path)\n#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n#     downsample_rate = max(1, video_fps // fps)\n    \n#     # Read frames and downsample\n#     frames = []\n#     for i in range(total_frames):\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n#         if i % downsample_rate == 0:  # Downsample\n#             frames.append(frame)\n#     cap.release()\n\n#     # Create overlapping segments\n#     frame_count = len(frames)\n#     segment_frames = int(segment_duration * fps)\n#     step_frames = int((segment_duration - overlap) * fps)\n\n#     segments = []\n#     for start in range(0, frame_count - segment_frames + 1, step_frames):\n#         segments.append(frames[start:start + segment_frames])\n\n#     return np.array(segments)  # Shape: (num_segments, segment_frames, H, W, C)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torchvision.models as models\n# from torchvision import transforms\n# from PIL import Image\n# import cv2\n\n# # Load a pre-trained ResNet50 model\n# emotion_model = models.resnet50(pretrained=True)\n# emotion_model.fc = torch.nn.Linear(emotion_model.fc.in_features, 8)  # Assume 8 emotion classes\n# emotion_model.eval()\n\n# # Transformation pipeline for input images\n# preprocess = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n\n# def extract_deep_emotional_features(frame):\n#     \"\"\"\n#     Extract deep emotional features from a single video frame.\n\n#     :param frame: Video frame (H, W, C) as a numpy array\n#     :return: Deep emotional features as a PyTorch tensor\n#     \"\"\"\n#     # Convert the frame to PIL Image\n#     image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n#     # Preprocess the image\n#     input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n\n#     # Extract features\n#     with torch.no_grad():\n#         features = emotion_model(input_tensor)\n#     return features.squeeze().numpy()  # Return as a numpy array\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def extract_features_from_segment(segment):\n#     \"\"\"\n#     Extract features from a single video segment.\n\n#     :param segment: A video segment (10 frames, H, W, C)\n#     :return: Deep emotional features and geometric features\n#     \"\"\"\n#     # Initialize Face Mesh model\n#     face_mesh = FaceMesh(static_image_mode=True)\n\n#     deep_features = []\n#     geometric_features = []\n\n#     for frame in segment:\n#         # Extract face landmarks\n#         results = face_mesh.process(frame)\n\n#         if results.multi_face_landmarks:\n#             landmarks = results.multi_face_landmarks[0]\n            \n#             # Extract deep emotional features using pre-trained model\n#             deep_feat = extract_deep_emotional_features(frame)  # Shape: (8,)\n#             deep_features.append(deep_feat)\n\n#             # Extract geometric features (example logic)\n#             geo_feat = extract_geometric_features(landmarks)\n#             geometric_features.append(geo_feat)\n#         else:\n#             # Handle case where no face is detected\n#             deep_features.append(np.zeros(8))  # Update size based on model output\n#             geometric_features.append(np.zeros(115))\n\n#     return np.array(deep_features), np.array(geometric_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import os\n\n# def save_features(features, video_id, output_dir, feature_type):\n#     \"\"\"\n#     Save extracted features as CSV files.\n\n#     :param features: Feature array (num_segments, frames, feature_dim)\n#     :param video_id: Unique video identifier\n#     :param output_dir: Directory to save features\n#     :param feature_type: Type of features (\"deep\" or \"geometric\")\n#     \"\"\"\n#     os.makedirs(output_dir, exist_ok=True)\n#     for i, segment_features in enumerate(features):\n#         segment_file = os.path.join(output_dir, f\"{video_id}_segment_{i}_{feature_type}.csv\")\n#         pd.DataFrame(segment_features).to_csv(segment_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output_dir = \"video_extracted_features\"\n# video_files = \"/kaggle/input/first-impression-v2-train-dataset/train-1/training80_01\"\n# for video_path in video_files:\n#     video_id = os.path.splitext(os.path.basename(video_path))[0]\n#     print(f\"Processing {video_id}...\")\n\n#     # Segment video\n#     segments = segment_video(video_path)\n\n#     deep_features_all = []\n#     geometric_features_all = []\n\n#     for segment in segments:\n#         deep_feat, geo_feat = extract_features_from_segment(segment)\n#         deep_features_all.append(deep_feat)\n#         geometric_features_all.append(geo_feat)\n\n#     # Save features\n#     save_features(deep_features_all, video_id, output_dir, \"deep\")\n#     save_features(geometric_features_all, video_id, output_dir, \"geometric\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ____________________________________________________________________ #\n\n# STARTING HERE\n","metadata":{}},{"cell_type":"code","source":"# ____________________________________________________________________ # here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch torchvision mediapipe opencv-python\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom mediapipe.python.solutions.face_mesh import FaceMesh\nfrom PIL import Image\nimport pandas as pd\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" \n\ndef segment_video(video_path, segment_duration=2, fps=1):\n    \"\"\"\n    Segments a video into non-overlapping chunks with skipped frames.\n\n    :param video_path: Path to the video file\n    :param segment_duration: Duration of each segment (in seconds)\n    :param fps: Frames per second to sample\n    :return: List of video segments as numpy arrays\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n    downsample_rate = max(1, video_fps // fps)\n\n    # Extract frames at 1 FPS\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % downsample_rate == 0:\n            frames.append(frame)\n    cap.release()\n\n    # Ensure the correct frame sampling\n    segment_frames = segment_duration * fps  # 2 frames for 2 seconds\n    segments = []\n    frame_indices = [1, 4, 7, 10, 13]  # Start frames for the 5 segments\n\n    for start in frame_indices:\n        if start + segment_frames <= len(frames):\n            segments.append(frames[start:start + segment_frames])\n\n    return np.array(segments)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load a pre-trained ResNet50 model\nemotion_model = models.resnet50(pretrained=True)\nemotion_model.fc = torch.nn.Linear(emotion_model.fc.in_features, 8) \nemotion_model.eval()\n\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef extract_deep_emotional_features(frame):\n    \"\"\"\n    Extract deep emotional features from a single frame.\n\n    :param frame: Frame as a numpy array (H, W, C)\n    :return: Emotion features as a numpy array\n    \"\"\"\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    input_tensor = preprocess(image).unsqueeze(0)\n    with torch.no_grad():\n        features = emotion_model(input_tensor)\n    return features.squeeze().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_geometric_features(landmarks):\n    \"\"\"\n    Extract geometric features from face landmarks.\n\n    :param landmarks: Face landmarks from MediaPipe FaceMesh\n    :return: Geometric features as a numpy array\n    \"\"\"\n    # Example: Extract (x, y) coordinates of specific points\n    coords = [(lm.x, lm.y) for lm in landmarks.landmark]\n    coords = np.array(coords).flatten()\n    return coords\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features_from_segment(segment):\n    \"\"\"\n    Extract features from a single video segment.\n\n    :param segment: Segment containing multiple frames\n    :return: Deep emotional features and geometric features\n    \"\"\"\n    face_mesh = FaceMesh(static_image_mode=True)\n    deep_features = []\n    geometric_features = []\n\n    for frame in segment:\n        results = face_mesh.process(frame)\n\n        if results.multi_face_landmarks:\n            landmarks = results.multi_face_landmarks[0]\n            deep_feat = extract_deep_emotional_features(frame)\n            geo_feat = extract_geometric_features(landmarks)\n        else:\n            deep_feat = np.zeros(8)  # Match size of deep features\n            geo_feat = np.zeros(468 * 2)  # Match size of flattened landmarks\n\n        deep_features.append(deep_feat)\n        geometric_features.append(geo_feat)\n\n    return np.array(deep_features), np.array(geometric_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(features, video_id, output_dir, feature_type):\n    \"\"\"\n    Save features to CSV.\n\n    :param features: Array of features\n    :param video_id: Video identifier\n    :param output_dir: Directory to save files\n    :param feature_type: Type of features (\"deep\" or \"geometric\")\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    for i, segment_features in enumerate(features):\n        file_name = os.path.join(output_dir, f\"{video_id}_segment_{i}_{feature_type}.csv\")\n        pd.DataFrame(segment_features).to_csv(file_name, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef list_videos(base_dir):\n    \"\"\"\n    List all video files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :return: List of file paths for all videos\n    \"\"\"\n    video_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith('.mp4'):\n                video_files.append(os.path.join(root, file))\n    return video_files\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Example usage\nbase_dir = \"/kaggle/input/fi-v2-validation-data/val-1/validation80_01\"\nvideo_files = list_videos(base_dir)\nprint(f\"Found {len(video_files)} video files.\")\n\noutput_dir = \"val_feature_extraction/val1_vf\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\noutput_file = \"val_feature_extraction/val1_vf/p1.csv\"\n\n# Initialize the output CSV with headers\ncolumns = ['Video_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(8)] + [f'Hc_{i}' for i in range(468 * 2)]\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n\nfor video_path in video_files:\n    video_id = os.path.splitext(os.path.basename(video_path))[0]\n    # print(f\"Processing {video_id}...\")\n\n    segments = segment_video(video_path)\n\n    for segment_id, segment in enumerate(segments):\n        # Extract features for the current segment\n        deep_feat, geo_feat = extract_features_from_segment(segment)\n        \n        # Combine features into a single row\n        combined_features = {\n            'Video_ID': video_id,\n            'Segment_ID': segment_id,\n        }\n        combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(8)})  # Aggregate deep features\n        combined_features.update({f'Hc_{i}': geo_feat.mean(axis=0)[i] for i in range(468 * 2)})  # Aggregate geometric features\n\n        # Convert to DataFrame and append to CSV\n        feature_row = pd.DataFrame([combined_features])\n        feature_row.to_csv(output_file, mode='a', header=False, index=False)\n\nprint(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")\n\n##################################################\\\\############WORKING","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef list_videos(base_dir):\n    \"\"\"\n    List all video files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :return: List of file paths for all videos\n    \"\"\"\n    video_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith('.mp4'):\n                video_files.append(os.path.join(root, file))\n    return video_files\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Example usage\nbase_dir = \"/kaggle/input/fi-v2-validation-data/val-1/validation80_02\"\nvideo_files = list_videos(base_dir)\nprint(f\"Found {len(video_files)} video files.\")\n\noutput_dir = \"val_feature_extraction/val1_vf\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\noutput_file = \"val_feature_extraction/val1_vf/p2.csv\"\n\n# Initialize the output CSV with headers\ncolumns = ['Video_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(8)] + [f'Hc_{i}' for i in range(468 * 2)]\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n\nfor video_path in video_files:\n    video_id = os.path.splitext(os.path.basename(video_path))[0]\n    # print(f\"Processing {video_id}...\")\n\n    segments = segment_video(video_path)\n\n    for segment_id, segment in enumerate(segments):\n        # Extract features for the current segment\n        deep_feat, geo_feat = extract_features_from_segment(segment)\n        \n        # Combine features into a single row\n        combined_features = {\n            'Video_ID': video_id,\n            'Segment_ID': segment_id,\n        }\n        combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(8)})  # Aggregate deep features\n        combined_features.update({f'Hc_{i}': geo_feat.mean(axis=0)[i] for i in range(468 * 2)})  # Aggregate geometric features\n\n        # Convert to DataFrame and append to CSV\n        feature_row = pd.DataFrame([combined_features])\n        feature_row.to_csv(output_file, mode='a', header=False, index=False)\n\nprint(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef list_videos(base_dir):\n    \"\"\"\n    List all video files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :return: List of file paths for all videos\n    \"\"\"\n    video_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith('.mp4'):\n                video_files.append(os.path.join(root, file))\n    return video_files\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Example usage\nbase_dir = \"/kaggle/input/fi-v2-validation-data/val-1/validation80_03\"\nvideo_files = list_videos(base_dir)\nprint(f\"Found {len(video_files)} video files.\")\n\noutput_dir = \"val_feature_extraction/val1_vf\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\noutput_file = \"val_feature_extraction/val1_vf/p3.csv\"\n\n# Initialize the output CSV with headers\ncolumns = ['Video_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(8)] + [f'Hc_{i}' for i in range(468 * 2)]\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n\nfor video_path in video_files:\n    video_id = os.path.splitext(os.path.basename(video_path))[0]\n    # print(f\"Processing {video_id}...\")\n\n    segments = segment_video(video_path)\n\n    for segment_id, segment in enumerate(segments):\n        # Extract features for the current segment\n        deep_feat, geo_feat = extract_features_from_segment(segment)\n        \n        # Combine features into a single row\n        combined_features = {\n            'Video_ID': video_id,\n            'Segment_ID': segment_id,\n        }\n        combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(8)})  # Aggregate deep features\n        combined_features.update({f'Hc_{i}': geo_feat.mean(axis=0)[i] for i in range(468 * 2)})  # Aggregate geometric features\n\n        # Convert to DataFrame and append to CSV\n        feature_row = pd.DataFrame([combined_features])\n        feature_row.to_csv(output_file, mode='a', header=False, index=False)\n\nprint(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # pip install rarfile\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import zipfile\n\n# def compress_folder_to_zip(folder_path, output_zip_path):\n#     # Check if the folder exists\n#     if not os.path.isdir(folder_path):\n#         raise ValueError(f\"Folder path '{folder_path}' does not exist.\")\n    \n#     # Ensure the output file has the `.zip` extension\n#     if not output_zip_path.endswith(\".zip\"):\n#         output_zip_path += \".zip\"\n\n#     # Create a ZIP file\n#     with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n#         for root, _, files in os.walk(folder_path):\n#             for file in files:\n#                 file_path = os.path.join(root, file)\n#                 # Add file to ZIP with a relative path\n#                 zipf.write(file_path, os.path.relpath(file_path, folder_path))\n\n#     print(f\"Folder successfully compressed into: {output_zip_path}\")\n\n# # Example usage\n# folder_path = \"/kaggle/working/feature_extraction\"  # Replace with your folder path on Kaggle\n# output_zip_path = \"/kaggle/working/fi_video_features\"  # Output file will be saved in the working directory\n\n# compress_folder_to_zip(folder_path, output_zip_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}