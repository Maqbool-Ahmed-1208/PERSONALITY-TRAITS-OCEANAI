{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10545318,"sourceType":"datasetVersion","datasetId":6524649},{"sourceId":10545342,"sourceType":"datasetVersion","datasetId":6524666}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VAL AUDIO EXTRACTION","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Delete a folder and its contents\nshutil.rmtree('/kaggle/working/fi_audio_TEST_dataset')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n# Path to the ZIP file\nzip_file_path = '/kaggle/input/fi-v2-test-data/test-2e.zip'\n\n# os.makedirs('test_set', exist_ok=True)\n\n# Destination folder for unzipped content\nunzip_destination = '/kaggle/working/unzipped_test_data'\n\n# Unzip the file\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall(unzip_destination,pwd=b'zeAzLQN7DnSIexQukc9W')\n\nprint(\"Unzipping complete!\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Directory where your ZIP files are located\ndirectory_path = '/kaggle/working/fi_test_data'\n\n# Destination folder for unzipped content\nunzip_destination = '/kaggle/working/fi_v2_test_data/'\n\n# Password for the ZIP file (if you know it)\npassword = b'.chalearnLAPFirstImpressionsSECONDRoundICPRWorkshop2016.'  # Make sure the password is in bytes format\n\n# Create destination folder if it doesn't exist\nos.makedirs(unzip_destination, exist_ok=True)\n\n# Loop through all files in the directory\nfor filename in os.listdir(directory_path):\n    if filename.endswith('.zip'):  # Check if the file is a ZIP file\n        zip_file_path = os.path.join(directory_path, filename)\n        \n        # Print the name of the ZIP file found\n        print(f\"Found ZIP file: {filename}\")\n        \n        try:\n            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n                # If the zip is password-protected, attempt to extract with the password\n                try:\n                    zip_ref.extractall(unzip_destination, pwd=password)\n                    print(f\"Successfully unzipped: {filename}\")\n                except RuntimeError as e:\n                    print(f\"Error extracting {filename} - Password might be incorrect: {e}\")\n        except zipfile.BadZipFile:\n            print(f\"Error: {filename} is not a valid ZIP file.\")\n        except Exception as e:\n            print(f\"An error occurred while processing {filename}: {e}\")\n\nprint(\"Unzipping process completed!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef zip_folder(folder_path, zip_file_path):\n    \"\"\"\n    Zips the contents of a folder.\n\n    Args:\n        folder_path (str): Path to the folder to be zipped.\n        zip_file_path (str): Destination path for the created ZIP file.\n    \"\"\"\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the folder\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                # Add file to the zip, maintaining folder structure\n                arcname = os.path.relpath(file_path, folder_path)\n                zipf.write(file_path, arcname)\n    print(f\"Folder '{folder_path}' has been zipped to '{zip_file_path}'\")\n\n# Example usage\nfolder_to_zip = '/kaggle/working/TEST_audio_feature_extraction'  # Folder you want to zip\ndestination_zip = '/kaggle/working/TEST_audio_feature_extraction.zip'  # Destination zip file path\n\nzip_folder(folder_to_zip, destination_zip)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\n\ndef list_videos(base_dir, extensions=('.mp4',)):\n    \"\"\"\n    List all video files in the dataset directory.\n    :param base_dir: Path to the dataset base directory\n    :param extensions: Tuple of file extensions to include\n    :return: List of file paths for all videos\n    \"\"\"\n    video_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extensions):\n                video_files.append(os.path.join(root, file))\n    return video_files\n\ndef extract_audio(video_files, output_base_dir):\n    \"\"\"\n    Extract audio from video files and save as .wav files.\n    :param video_files: List of video file paths\n    :param output_base_dir: Base directory for saving audio files\n    \"\"\"\n    for video_file in video_files:\n        # Generate output directory and file paths\n        relative_path = os.path.relpath(video_file, input_dir)\n        output_dir = os.path.join(output_base_dir, os.path.dirname(relative_path))\n        os.makedirs(output_dir, exist_ok=True)\n        output_file = os.path.join(output_dir, os.path.basename(video_file).replace('.mp4', '.wav'))\n        \n        # Run FFmpeg to extract audio\n        subprocess.run(['ffmpeg', '-i', video_file, '-vn', '-acodec', 'pcm_s16le', '-ar', '44100', '-ac', '2', output_file])\n        print(f\"Extracted: {video_file} -> {output_file}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base directories\ninput_dir = '/kaggle/working/fi_v2_test_data'\noutput_base_dir = '/kaggle/working/fi_audio_TEST_dataset'\n\n# List video files\nvideo_files = list_videos(input_dir)\n\nprint(f\"Found {len(video_files)} video files.\")\nextract_audio(video_files, output_base_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"# import os\n# import shutil\n\n# # Specify the folder to delete\n# folder_path = \"/kaggle/working/segmented_audio\"\n\n# # Check if the folder exists\n# if os.path.exists(folder_path):\n#     # Delete the folder and its contents\n#     shutil.rmtree(folder_path)\n#     print(f\"Folder '{folder_path}' has been deleted.\")\n# else:\n#     print(f\"Folder '{folder_path}' does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Specify the file to delete\n# file_path = \"/kaggle/working/audio_feature_extraction/train_6_af.csv\"\n\n# # Check if the file exists\n# if os.path.exists(file_path):\n#     # Delete the file\n#     os.remove(file_path)\n#     print(f\"File '{file_path}' has been deleted.\")\n# else:\n#     print(f\"File '{file_path}' does not exist.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_TEST_dataset\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"TEST_audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"TEST_audio_feature_extraction/FI_TEST_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"audio_feature_extraction/train_2_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_train_dataset/train-3\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"audio_feature_extraction/train_3_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_train_dataset/train-4\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"audio_feature_extraction/train_4_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_train_dataset/train-5\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"audio_feature_extraction/train_5_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DEEP AND HC FEATURE EXTRACTION val","metadata":{}},{"cell_type":"code","source":"import os\n\ndef list_audio_files(base_dir, extension=\".wav\"):\n    \"\"\"\n    List all audio files in the dataset directory.\n\n    :param base_dir: Path to the dataset base directory\n    :param extension: File extension to look for (default is \".wav\")\n    :return: List of file paths for all audio files\n    \"\"\"\n    audio_files = []\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(extension):\n                audio_files.append(os.path.join(root, file))\n    return audio_files\n\n# Example usage\nbase_dir = \"/kaggle/working/fi_audio_val_dataset/val-2\"  # Replace with the path to your dataset\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\noutput_dir = \"val_audio_feature_extraction\"\n\n# Create output directory if not exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Define output file and columns\noutput_file = \"val_audio_feature_extraction/val_2_af.csv\"\ncolumns = ['Audio_ID', 'Segment_ID'] + [f'Deep_{i}' for i in range(512)] + [f'Hc_{i}' for i in range(25)]\n\n# Initialize the CSV if it doesn't exist\nif not os.path.exists(output_file):\n    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef segment_audio(audio_path, segment_duration=2, skip_duration=3):\n    audio = AudioSegment.from_file(audio_path)\n    segments = []\n    audio_duration = len(audio) / 1000  # in seconds\n    for start in range(0, int(audio_duration - segment_duration), skip_duration):  # Segment based on audio length\n        segment = audio[start * 1000:(start + segment_duration) * 1000]  # Segment duration in milliseconds\n        if len(segment) < segment_duration * 1000:\n            segment = segment + AudioSegment.silent(duration=(segment_duration * 1000 - len(segment)))  # Pad with silence\n        segments.append(segment)\n    return segments\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# Load pretrained emotional VGG-16 model\nvgg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg_model.eval()\n\ndef extract_deep_features(segments):\n    features = []\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    for segment in segments:\n        # Compute log-Mel spectrogram\n        samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n        mel_spec = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128, fmax=8000, hop_length=512, win_length=2048)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\n        # Normalize the spectrogram to range 0-255 for image conversion\n        log_mel_spec = np.clip(log_mel_spec, a_min=-80, a_max=0)  # Log scale\n        log_mel_spec = (log_mel_spec + 80) * (255.0 / 80.0)  # Normalize to [0, 255]\n\n        # Convert to image and resize\n        image = Image.fromarray(log_mel_spec.astype(np.uint8)).convert(\"RGB\")\n        image = transform(image).unsqueeze(0)\n\n        # Extract features using VGG-16\n        with torch.no_grad():\n            deep_feature = vgg_model(image).numpy()\n        features.append(deep_feature)\n\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install opensmile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opensmile\nfrom opensmile import Smile, FeatureSet, FeatureLevel\n\nsmile = Smile(\n    feature_set=FeatureSet.eGeMAPSv02,\n    feature_level=FeatureLevel.Functionals\n)\n\ndef extract_egemaps(segment):\n    \"\"\"\n    Extract eGeMAPS features using openSMILE.\n\n    :param segment: Audio segment\n    :return: Hand-crafted features as a numpy array\n    \"\"\"\n    samples = np.array(segment.get_array_of_samples(), dtype=np.float32)\n    features = smile.process_signal(samples, sampling_rate=16000).to_numpy()\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_features(audio_id, segment_id, deep_feat, hc_feat):\n    \"\"\"\n    Save the extracted features to the CSV file.\n\n    :param audio_id: ID of the audio file\n    :param segment_id: ID of the audio segment\n    :param deep_feat: Deep features for the segment\n    :param hc_feat: Hand-crafted features for the segment\n    \"\"\"\n    # Aggregate features (e.g., mean)\n    combined_features = {\n        'Audio_ID': audio_id,\n        'Segment_ID': segment_id,\n    }\n    combined_features.update({f'Deep_{i}': deep_feat.mean(axis=0)[i] for i in range(512)})\n    combined_features.update({f'Hc_{i}': hc_feat.mean(axis=0)[i] for i in range(25)})\n\n    # Append to CSV\n    feature_row = pd.DataFrame([combined_features])\n    feature_row.to_csv(output_file, mode='a', header=False, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_files):\n    for audio_path in audio_files:\n        audio_id = os.path.splitext(os.path.basename(audio_path))[0]\n        print(f\"Processing {audio_id}...\")\n\n        # Segment the audio\n        segments = segment_audio(audio_path)\n\n        for segment_id, segment in enumerate(segments):\n            # Extract features\n            deep_feat = extract_deep_features([segment])[0]\n            hc_feat = extract_egemaps(segment)\n\n            # Save features to CSV\n            save_features(audio_id, segment_id, deep_feat, hc_feat)\n\n    print(f\"All features saved to {output_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: List all audio files in the dataset directory\n# base_dir = \"/kaggle/working/fi_audio_train_dataset/train-2\"  # Replace with your path\naudio_files = list_audio_files(base_dir)\nprint(f\"Found {len(audio_files)} audio files.\")\n\n# Step 2: Process all audio files\nprocess_audio_files(audio_files)  # This function is already defined in the previous pipeline\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef compress_folder_to_zip(folder_path, output_zip_path):\n    # Check if the folder exists\n    if not os.path.isdir(folder_path):\n        raise ValueError(f\"Folder path '{folder_path}' does not exist.\")\n    \n    # Ensure the output file has the `.zip` extension\n    if not output_zip_path.endswith(\".zip\"):\n        output_zip_path += \".zip\"\n\n    # Create a ZIP file\n    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                # Add file to ZIP with a relative path\n                zipf.write(file_path, os.path.relpath(file_path, folder_path))\n\n    print(f\"Folder successfully compressed into: {output_zip_path}\")\n\n# Example usage\nfolder_path = \"/kaggle/working/val_audio_feature_extraction\"  # Replace with your folder path on Kaggle\noutput_zip_path = \"/kaggle/working/val_audio_feature_extraction.zip\"  # Output file will be saved in the working directory\n\ncompress_folder_to_zip(folder_path, output_zip_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}